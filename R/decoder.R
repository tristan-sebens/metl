#' Decoder base class
#'
#' Logical representation of a single tag, which contains all necessary logic to parse and extract all required data from an appropriately formatted directory. Contains one or more [DataMap] objects, each of which the Decoder will apply in turn to the directory in order to extract the necessary data. Contains an [Identifier] object, which can be used to automatically determine if a given directory contains the data generated by a tag of the make/model to which this Decoder refers.
#'
#' @field data_maps list. The list of data maps used to map the data for this tag to the output tables in the DB
#'
#' @export Decoder
Decoder =
  #----
setRefClass(
  "Decoder",
  fields =
    list(
      label = "character",
      identifier = "Identifier",
      data_maps = "list",
      output_fieldmaps = "list",
      user_input_fieldmap = "FieldMap",
      field_metadata_fieldmap = "FieldMap"
    ),

  methods =
    list(
      initialize =
        function(
          output_fieldmaps =
            list(
              "meta" = ABLTAG_METADATA_TABLE_FIELDS,
              "instant" = ABLTAG_DATA_INSTANT_TABLE_FIELDS,
              "summary" = ABLTAG_DATA_SUMMARY_TABLE_FIELDS,
              "histogram_meta" = ABLTAG_HISTOGRAM_METADATA_TABLE_FIELDS,
              "histogram" = ABLTAG_HISTOGRAM_DATA_TABLE_FIELDS,
              "pdt" = ABLTAG_PDT_DATA_TABLE_FIELDS,
              "input" = ABLTAG_USER_INPUT_FIELDS,
              "field_meta" = ABLTAG_FIELD_METADATA_TABLE_FIELDS
            ),
          user_input_fieldmap = USER_INPUT_FIELDS,
          field_metadata_fieldmap = AUTOGENERATED_FIELD_METADATA_TABLE_FIELDS,
          ...
        ) {
          callSuper(
            output_fieldmaps = output_fieldmaps,
            user_input_fieldmap = user_input_fieldmap,
            field_metadata_fieldmap = field_metadata_fieldmap,
            ...
          )
        },

      # Helper function to throw an error with a pre-appended message to help identify
      # the source of the error
      throw_error =
        function(msg) {
          stop(
            paste0(
              "ERROR - ",
              class(.self)[[1]],
              ": ",
              msg
            )
          )
        },

      create_userinput_datamap =
        function(meta) {
          DataMap(
            input_data_field_map = user_input_fieldmap,
            extract_fn =
              function(d) {
                return(meta)
              }
          )
        },

      create_field_meta_datamap =
        function() {
          field_metadata =
            build_field_metadata_frame(.self)

          # dbplyr has a limit to how many characters a field can contain
          # Check to ensure that none of the field descriptions exceed this limit
          field_descriptions =
            field_metadata[[
              field_metadata_fieldmap$field_list$FIELD_DESCRIPTION_FIELD$name
            ]]

          too_long =
            which(nchar(field_descriptions) > 255)

          if (length(too_long) > 0)
            stop(
              paste0(
                c(
                  "ERROR: The descriptions for the following fields exceed the 255 character limit:",
                  paste0(field_metadata$table[too_long], ".", field_metadata$field[too_long]),
                  "Please adjust the descriptions in the Field configurations to be less than 255 characters"
                ),
                collapse = "\n"
              )
            )

          return(
            DataMap(
              input_data_field_map = field_metadata_fieldmap,
              extract_fn =
                function(d) {
                  return(field_metadata)
                }
            )
          )
        },

      # Execute all necessary steps to read and transform raw data for one DataMap
      decode_datamap =
        function(d, dm, op_fm) {
          "Execute all necessary steps to read and transform raw data for one DataMap"
          # Perform initial extraction
          dat_e =
            dm$extract(d)

          # Transform extracted data
          dat_et =
            dm$transform(dat_e, op_fm)

          # Return transformed data
          return(dat_et)
        },

      add_missing_fields =
        function(dat1, dat1_ip_fm, dat1_op_fm, dat2, dat2_ip_fm, dat2_op_fm) {
          "Add fields from one data frame to a second dataframe, based on their
            respective DataMaps. dat_1/dm_1 refer to the data which may be missing
            fields. dat_2/dm_2 refer to the data from which we MAY take data to
            put into dat_1. Any fields defined by dm_1$output_data_field_map which
            are missing from dat_1 but are present in dat_2 will be added to dat_1."
          # DM 1 is the DataMap which we are working on
          # DM 2 is the DataMap which we MAY take data from to put into the data for DM 1

          # If dat1 is emtpy, there's nothing to do.
          if(nrow(dat1) == 0) {return(dat1)}

          # First find any fields in dat1 which are present in the output FieldMap
          # but missing in the input FieldMap
          missing_fields = names(dat1_op_fm$uncommon_fields(dat1_ip_fm))

          # Next determine if any of the missing fields are references to fields in
          # the output FieldMap of dat2

          # Collect all of the UUIDs of the output FieldMap for dat2
          dat2_op_uids =
            dat2_op_fm$field_list %>%
            lapply(function(f) {f$uid}) %>%
            unlist(use.names = F)

          matching_fields =
            Filter(
              f =
                function(e) {
                  dat1_op_fm$field_list[[e]]$uid %in% dat2_op_uids
                },
              x =
                missing_fields
            )

          # Finally, if there are matching fields, check if those fields are also
          # present in the input FieldMap of dat2. If they aren't then that means
          # these fields are the result of a prior completion just like this one.
          available_fields =
            Filter(
              f =
                function(e) {
                  e %in% names(dat2_ip_fm$field_list)
                },
              x =
                matching_fields
            )

          # If such fields exist, this means that there are fields which are missing
          # from dat1, which it did not originally have access to (as indicated by
          # those fields being absent from the input FieldMap), but which it is
          # supposed to have (indicated by the presence of those fields in the
          # output FieldMap), and which are available from dat2, which dat2 was not
          # given from another source (as indicated by the presence of the field in
          # the input FieldMap of dat2).

          # Now we simply iterate through those available fields and add them from
          # dat2 to dat1. We assume that both datasets have been transformed at this
          # point, so fieldnames will be taken from the output FieldMaps for both

          # For each missing field which is available in the second DataMap, add the appropriate data
          for(f_ in available_fields) {
            dat1_field_name = dat1_op_fm$field_list[[f_]]$name
            dat2_field_name = dat2_op_fm$field_list[[f_]]$name

            incoming_dat = dat2[[dat2_field_name]]

            if(length(incoming_dat) == 1) {
              # If there is only a single value being brought in from dat2, we can perform a direct insert
              dat1[[dat1_field_name]] = incoming_dat
            } else {
              # If the incoming data has multiple values, then we need to check if dat1 also has multiple values
              if(nrow(dat1) == 1) {
                if(length(unique(incoming_dat)) == 1) {
                  # If the vector is composed entirely of repeats of a single value, then the expanded dataframe would be meaningless and we can truncate it here
                  dat1[dat1_field_name] = unique(incoming_dat)
                } else {
                  # If there are multiple unique values, then we can perform a full join
                  dplyr::cross_join(
                    dat1,
                    incoming_dat
                  )
                }
              } else {
                # If both dat1 and incoming_data have multiple rows, then the only case for which behavior is defined is if they have the same number of rows
                if(nrow(dat1) == length(incoming_dat)) {
                  dat1[[dat1_field_name]] = incoming_dat
                } else {
                  throw_error(
                    paste0(
                      "Error while adding data from ",
                      dat2_field_name,
                      " to ",
                      dat1_field_name,
                      ". Both fields have multiple rows, but not the same number of rows, so behavior is undefined."
                    )
                  )
                }
              }
            }
          }

          return(dat1)
        },

      # Complete each dataframe with fields it is missing (according to its
      # FieldMap) which are available in the other dataframes
      complete_dataframes =
        function(dfs, ip_fms, op_fms) {
          # Calculate a comparison grid, identifying all distinct
          # pairings of the incoming datasets
          ix_mx =
            expand.grid(
              d1 = seq(1, length(dfs)),
              d2 = seq(1, length(dfs))
            ) %>%
            dplyr::filter(d1 != d2) %>%
            plyr::arrange(d1, d2) %>%
            as.matrix

          # For each pairing, execute the 'add_missing_fields'
          # method, completing the datasets
          for (i in seq(1, nrow(ix_mx))) {
            dat1_ix = ix_mx[[i, 1]]
            dat2_ix = ix_mx[[i, 2]]


            dfs[[dat1_ix]] =
              add_missing_fields(
                dat1 = dfs[[dat1_ix]],
                dat1_ip_fm = ip_fms[[dat1_ix]],
                dat1_op_fm = op_fms[[dat1_ix]],
                dat2 = dfs[[dat2_ix]],
                dat2_ip_fm = ip_fms[[dat2_ix]],
                dat2_op_fm = op_fms[[dat2_ix]]
              )
          }

          return(dfs)
        },

      upsert =
        function(con, dat, output_data_field_map) {
          # If the incoming data.frame is empty, there is nothing to upsert
          if(nrow(dat) == 0) return()
          # Generate a temporary table name
          temp_table_name =
            # Generate a name for the temporary table by appending a random string
            dbplyr::ident(
              paste0(
                output_data_field_map$table,
                "_temp_",
                stringi::stri_rand_strings(1, 12, pattern="[A-Za-z]") # Random alphanumeric to avoid clobbering
              )
            )

          # Generate a properly formatted identifier of the output table
          output_table_name =
            dbplyr::ident(
              output_data_field_map$table
            )

          # Execute the whole upsert in a try/catch statment so that even if an
          # error is encountered, we can ensure the tempoary table gets dropped.
          tryCatch(
            expr =
              {
                # Try to create the temporary table
                tryCatch(
                  {
                    # There's an odd message that keeps cropping up here. It seems to
                    # occur the first time that anything is loaded into the DB, and it
                    # occurs within this function call. The message says:
                    #  Note: method with signature ‘Oracle#character’ chosen for
                    #  function ‘odbcConnectionColumns_’, target signature ‘Oracle#SQL’.
                    #  "OdbcConnection#SQL" would also be valid.
                    #
                    # Some quick research indicates that this is a known issue buried
                    # somewhere within the dbplyr package (https://forum.posit.co/t/what-does-this-dbwritetable-message-mean/10690/1),
                    # and that, most importantly, it's probably not the fault of me or
                    # my code. For now, I'm just wrapping the call in this
                    # suppression clause to prevent the message from messing up the
                    # package output.
                    # suppressMessages(
                    #   {
                        # Copy data to temporary table
                        dbplyr::db_copy_to(
                          con = con,
                          # table = dbplyr::ident(temp_table_name),
                          table = temp_table_name,
                          # All of these calls also produce the message, so it's unclear what to do to stop it
                          # table = dbplyr::sql(temp_table_name),
                          # table = I(temp_table_name),
                          # table = temp_table_name,
                          values = dat,
                          # Already in a transaction, so this should not start its own transaction
                          in_transaction = F,
                          # Typically this function writes to a temporary table which
                          # should then be automatically dropped when the connection is
                          # severed. However, at least in Oracle, this doesn't happen.
                          # Additionally, when a table has the TEMPORARY token
                          # it sort of locks it into appearing to be 'in use' so long
                          # as the original connection is still open. This problem
                          # isn't shared by standard tables, so I've elected to just
                          # use a standard table in a temporary manner, and that seems
                          # to solve the issue. All this really means is that we have to
                          # EXPLICITLY delete the table when we're done with it.
                          temporary = F
                        )
                    #   }
                    # )
                  },

                  error =
                    function(cond) {
                      # Raise the error which caused the problem
                      stop(
                        paste0(
                          c(
                            paste0(
                              "Error creating temporary table ",
                              temp_table_name,
                              ":"
                            ),
                            get_cond_stack_messages(cond = cond)
                          ),
                          collapse = "\n"
                        )
                      )
                    }
                )

                # Determine fields used to identify individual records
                id_fs = output_data_field_map$get_id_field_names()

                # Try to merge the new data into the target table
                tryCatch(
                  expr =
                    {
                      # UPSERT the new data into the target table
                      DBI::dbExecute(
                        con = con,
                        statement =
                          # Build the upsert sql statement
                          dbplyr::sql_query_upsert(
                            con = con,
                            table = output_table_name,
                            from = temp_table_name,
                            # Fields used to find unique records
                            by = id_fs,
                            # Fields to be updated (non-id fields)
                            update_cols = names(dat)[!names(dat) %in% id_fs]
                          )
                      )
                    },
                  error =
                    function(cond) {
                      stop(
                        paste0(
                          c(
                            paste0(
                              "Error updating ",
                              output_data_field_map$table,
                              " from temporary table:"
                            ),
                            get_cond_stack_messages(cond = cond)
                          ),
                          collapse = "\n"
                        )
                      )
                    }
                )
              },
            # Ensure that the temporary table is deleted.
            finally =
              {
                # Check if the table was created in the DB. If it was, drop it.
                if(DBI::dbExistsTable(con, temp_table_name)) {
                  DBI::dbRemoveTable(con, temp_table_name)
                }
              }
          )
        },

      verify_data_directory =
        function(d) {
          # Check if the given directory exists. If not, throw an appropriate error
          if(!dir.exists(d)) {
            stop("The selected directory does not exist.")
          }

          # Check if the directory positively identifies with this Decoder's Identifier
          # If not, it is still possible that the Decoder will be able to
          # import from the directory, but the user should be warned that the
          # directory does not match the expected structure.
          if(!identifier$identify(d))
            warning(
              paste0(
                c(
                  "The selected directory does not show the structure expected by the selected Decoder:",
                  "",
                  identifier$failed_condition_messages(d)
                ),
                collapse = "\n"
              )
            )

          if(!identifier$valid(d)) {
            stop("The selected directory does not contain the necessary data to proceed.")
          }
        },

      expand_data_maps =
        function(meta) {
          data_maps_expanded = data_maps

          if(!nrow(meta) == 0) {
            # Create a stub DataMap to return the user-inputted data
            DataMap_UserInput =
              create_userinput_datamap(meta)

            data_maps_expanded =
              append(data_maps, list("input" = DataMap_UserInput))
          }

          if("field_meta" %in% names(output_fieldmaps)) {
            # Create a stub DataMap to return the Field metadata
            DataMap_FieldMetaData =
              create_field_meta_datamap()

            data_maps_expanded =
              append(data_maps_expanded, list("field_meta" = DataMap_FieldMetaData))
          }

          return(data_maps_expanded)

        },

      decode =
        function(d, meta) {
          verify_data_directory(d)

          # Expand data_maps with DataMap objects for auto-generated data
          data_maps_expanded = expand_data_maps(meta)

          # Initialize an empty list to hold data
          decoded_data_list = list()
          # Iterate over each data map in the decoder's data_maps list
          for (data_type in names(data_maps_expanded)) {
            print(data_type)
            decoded_data =
              decode_datamap(
                dm = data_maps_expanded[[data_type]],
                d = d,
                op_fm = output_fieldmaps[[data_type]]
              )

            # Add the decoded data to the list
            decoded_data_list[[data_type]] = decoded_data
          }

          if(length(data_maps_expanded) > 1) {
            # Complete the dataframes with fields from each other as necessary
            completed_data =
              complete_dataframes(
                dfs =
                  decoded_data_list,
                ip_fms =
                  lapply(data_maps_expanded[names(data_maps_expanded)], function(dm) dm$input_data_field_map),
                op_fms =
                  output_fieldmaps[names(data_maps_expanded)]
              )
          } else {
            completed_data = decoded_data_list
          }

          names(completed_data) = names(decoded_data_list)

          # Remove the user-input data from the list of decoded data.frames
          completed_data = completed_data[!names(completed_data) %in% c("input")]

          # Return the completed data
          return(completed_data)
        },

      decode_to_dataframes =
        function(...) {
          return(decode(...))
        },

      decode_to_csv =
        function(..., op_d) {
          decoded_data =
            decode_to_dataframes(...)

          for (data_type in names(decoded_data)) {
            o_fp = file.path(op_d, paste0(data_type, '.csv'))
            dat = decoded_data[[data_type]]
            write.csv(x = dat, file = o_fp, row.names = F)
          }
        },

      decode_to_db =
        function(..., con) {
          # Extract the data from the directory
          decoded_data =
            decode_to_dataframes(...)

          # Start a DB transaction within which each data.frame will be upserted
          # This way, if for some reason one data.frame fails to upsert, the
          # transaction will be rolled back and no data will be saved.
          DBI::dbWithTransaction(
            conn = con,
            code = {
              # Iterate over each data.frame in the decoded data
              for (data_type in names(decoded_data)) {
                # Get the output FieldMap for the current data type
                output_data_field_map = output_fieldmaps[[data_type]]
                # Get the data.frame for the current data type
                dat = decoded_data[[data_type]]
                # Perform the upsert
                upsert(
                  con=con,
                  dat=dat,
                  output_data_field_map=output_data_field_map
                )
              }
            }
          )
        }
    )
)
